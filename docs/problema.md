# Problema
En la actualidad, el análisis automático de emociones ha ganado relevancia en sectores como la publicidad, la salud mental, la educación y el entretenimiento. Particularmente en la industria del cine, comprender las reacciones emocionales de los espectadores ante determinados estímulos visuales puede aportar información valiosa para el desarrollo de contenido más atractivo y personalizado (Liao, Hou, & Tseng, 2024). No obstante, los métodos tradicionales de recolección de estas emociones (encuestas post-función, entrevistas, etc.) resultan ser poco precisos, no escalables y altamente dependientes de la memoria y predisposición del espectador. Esta limitación impide una evaluación en tiempo real y en condiciones naturales, donde las emociones espontáneas son clave (Shukla, Kumari, & Bhargavi, 2024).

Para solventar esta situación, se requiere el desarrollo de un sistema embebido que, de manera autónoma y en tiempo real, capture y clasifique las expresiones faciales de los espectadores durante la proyección de una película. Este sistema deberá operar bajo un esquema de edge computing (Edge AI), procesando localmente la información capturada para minimizar la latencia y proteger la privacidad del usuario. El reto técnico incluye integrar hardware compacto (Raspberry Pi), herramientas de visión por computador (OpenCV), modelo de redes neuronales optimizadas para dispositivos embebidos (TensorFlow Lite) y una imagen de sistema operativo personalizada generada con Yocto Project, que contemple todas las dependencias y configuraciones necesarias para la operación (Turabzadeh, Meng, Swash, Pleva, & Juhar, 2017).

El sistema debe reconocer al menos seis emociones básicas: enojo, felicidad, tristeza, miedo, disgusto y sorpresa. También debe registrar la actividad detectada mediante estampas de tiempo y transmitir los datos de forma segura hacia un sistema remoto o local para su posterior análisis (Srinivas, Saurav, Nayak, & Murukessan, 2021). Todo este procesamiento debe llevarse a cabo localmente para minimizar la latencia y optimizar el uso del ancho de banda.

Este proyecto representa un caso de estudio realista y desafiante en el diseño de soluciones inteligentes, distribuidas y embebidas, con aplicación directa en contextos del mundo real como las salas de cine, y potencial de escalabilidad hacia otros entornos sensibles al análisis emocional.

## Referencias
- T. -L. Liao, Y. -Y. Hou and Y. -H. Tseng, "Movie Recommendation System Based on Facial Emotion Recognition," 2024 IEEE International Conference on Future Machine Learning and Data Science (FMLDS), Sydney, Australia, 2024, pp. 141-146, doi: 10.1109/FMLDS63805.2024.00036.
- D. Shukla, R. Kumari and A. Bhargavi, "Human Face Detection and Emotion Recognition Using OpenCV through AI," 2024 12th International Conference on Internet of Everything, Microwave, Embedded, Communication and Networks (IEMECON), Jaipur, India, 2024, pp. 1-5, doi: 10.1109/IEMECON62401.2024.10845980.
- S. Turabzadeh, H. Meng, R. M. Swash, M. Pleva and J. Juhar, "Real-time emotional state detection from facial expression on embedded devices," 2017 Seventh International Conference on Innovative Computing Technology (INTECH), Luton, UK, 2017, pp. 46-51, doi: 10.1109/INTECH.2017.8102423.
- M. Srinivas; Sanjeev Saurav; Akshay Nayak; A. P. Murukessan, "Facial Expression Recognition Using Fusion of Deep Learning and Multiple Features," in Machine Learning Algorithms and Applications , Wiley, 2021, pp.229-246, doi: 10.1002/9781119769262.ch13.