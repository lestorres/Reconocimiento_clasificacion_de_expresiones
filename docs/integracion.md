# Integraci√≥n

El diagrama de la arquitectura integrada de software y hardware es: 

```mermaid
flowchart TB
    subgraph Espectador
        P[üòÉ Reacci√≥n del Espectador]
    end

    subgraph Camara USB
        A[üé• Captura de im√°genes]
    end

    subgraph Raspberry Pi 5
        B[üß† Preprocesamiento OpenCV]
        C[üîç Inferencia con TFLite]
        D[üè∑Ô∏è Clasificaci√≥n con Modelo ]
        E[üíæ Almacenamiento local]
        F[üì° Env√≠o de datos ]

        subgraph Sistema Yocto
            Y1[meta-poky]
            Y2[meta-yocto-bsp]
            Y3[meta-raspberrypi]
            Y4[meta-openembedded]
            Y5[meta-mylayer]
        end
    end

    subgraph Servidor Central
        G[üñ•Ô∏è Visualizaci√≥n resultados]
        H[üß© Interfaz de control]
        I[üîÅ Control de ciclo remoto]
    end

    subgraph Operador
        O[üë§ Interacci√≥n y monitoreo]
    end

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> B
    O --> H
    P --> A

    %% Colores suaves y distintos para cada capa
    style Y1 fill:#dbeafe,stroke:#3b82f6,color:#1e3a8a
    style Y2 fill:#fef3c7,stroke:#f59e0b,color:#78350f
    style Y3 fill:#ede9fe,stroke:#8b5cf6,color:#4c1d95
    style Y4 fill:#dcfce7,stroke:#22c55e,color:#14532d
    style Y5 fill:#ffe4e6,stroke:#e11d48,color:#881337,font-weight:bold



```


## üß© Descripci√≥n del proceso de integraci√≥n hardware/software
La soluci√≥n implementada combina de forma coordinada componentes de hardware y software para lograr un sistema funcional de detecci√≥n y visualizaci√≥n de emociones. A continuaci√≥n se detalla c√≥mo se integran y complementan estos componentes en cada etapa del proceso:

- üòÉ Reacci√≥n del Espectador: Es la reacci√≥n o la expresi√≥n de la emoci√≥n por parte del espectador

- üü¶ C√°mara USB (hardware): Es el punto de entrada del sistema. Captura im√°genes en tiempo real del espectador o usuario cuya expresi√≥n emocional ser√° analizada.

- üü© Raspberry Pi 5 (hardware + software): Act√∫a como nodo de procesamiento en el borde. Recibe la imagen desde la c√°mara y ejecuta varios procesos:

  - OpenCV (software): Realiza el preprocesamiento de la imagen (detecci√≥n de rostro, recorte, escalado y normalizaci√≥n).

  - TensorFlow Lite (software): Ejecuta el modelo de aprendizaje autom√°tico entrenado para clasificar emociones en tiempo real.

  - Red de comunicaci√≥n (hardware/software): Se utiliza WiFi o Ethernet para transferir los resultados inferidos (emociones y timestamps) al servidor central.

- üü• Servidor o computador Central (hardware + software): Recibe los datos procesados desde la Raspberry Pi y permite la interacci√≥n con el operador:

  - Interfaz Gr√°fica (software): Presenta visualmente las emociones detectadas y permite al operador ajustar par√°metros del sistema (por ejemplo, encender/apagar el ciclo de inferencia o cambiar umbrales de confianza).

  - Control remoto del ciclo de procesamiento (software): El operador puede modificar el comportamiento del sistema en tiempo real, enviando instrucciones de vuelta a la Raspberry Pi.

Este dise√±o modular e integrado permite una interacci√≥n fluida entre hardware y software: la Raspberry Pi se encarga de la inferencia local, optimizando la latencia y reduciendo la carga sobre la red, mientras que el servidor central act√∫a como centro de control y visualizaci√≥n, facilitando ajustes remotos y monitoreo continuo.
