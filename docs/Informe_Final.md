# Justificaci√≥n

El proyecto propuesto tiene un gran potencial para mejorar la experiencia del espectador en las salas de cine. La aplicaci√≥n permitir√≠a recopilar datos sobre las reacciones emocionales de los espectadores ante diferentes tipos de escenas, lo que facilitar√≠a el entrenamiento de nuevos modelos de aprendizaje autom√°tico m√°s precisos y eficientes. Eso ser√≠a posible mediante esta adaptabilidad, ya que ayudar√≠a a ofrecer recomendaciones personalizadas, como propuestas de pel√≠cula o cambio de g√©nero, basadas en el estado emocional del espectador. Es decir, si el sistema detectara se√±ales de aburrimiento o desinter√©s, podr√≠a proponer una alternativa m√°s pr√≥xima a sus gustos, con lo que su nivel de satisfacci√≥n aumentar√≠a [5].

Desde una perspectiva social, este proyecto tiene implicaciones significativas en t√©rminos de inclusi√≥n, bienestar y accesibilidad. La capacidad de detectar emociones en tiempo real puede utilizarse para crear entornos m√°s emp√°ticos y receptivos, especialmente en contextos donde la expresi√≥n emocional puede estar limitada o inhibida [6]. Por ejemplo, personas con condiciones que dificulten la comunicaci√≥n verbal podr√≠an beneficiarse de sistemas que interpreten su estado emocional y ajusten la interacci√≥n del entorno en consecuencia [7].

Desde el punto de vista t√©cnico el principal reto consiste en integrar diversos componentes tecnol√≥gicos en un sistema compacto y eficiente de una Raspberry Pi. Esta se encargar√° de ejecutar las herramientas de visi√≥n por computadora con la ayuda de OpenCV, para procesar las im√°genes en tiempo real. Adem√°s, se implementar√° un modelo de redes neuronales optimizado para dispositivos embebidos mediante TensorFlow Lite, lo que permite realizar el procesamiento local sin comprometer el rendimiento [8].

El sistema debe ser capaz de reconocer seis emociones b√°sicas: ira, alegr√≠a, tristeza, miedo, asco y sorpresa. Se logra entrenar al realizarlo en redes neuronales profundas en bases de datos de im√°genes faciales con gran volumen y etiquetadas. Para garantizar un procesamiento eficiente en dispositivos de bajo consumo, el modelo de red neuronal ser√° optimizado o recortado para ejecutarse en dispositivos como la Raspberry Pi [9] que en muchos casos conlleva a consecuencias como la reducci√≥n la precisi√≥n o certeza del modelo.

Por otro lado, como norma se debe asegurar que el sistema cuente con mecanismos de seguridad y privacidad que garanticen que los datos capturados sean procesados de manera segura. Esto implica el uso de t√©cnicas de encriptaci√≥n para transmitir la informaci√≥n de manera protegida hacia un sistema de an√°lisis local [10]. Adem√°s, la personalizaci√≥n del sistema operativo a trav√©s del uso de Yocto Project facilitar√° la creaci√≥n de una imagen de sistema operativo optimizada y espec√≠fica para el hardware de la Raspberry Pi 5, garantizando que todas las dependencias necesarias para el correcto funcionamiento del sistema [11].

La integraci√≥n de sistemas de reconocimiento emocional en entornos como las salas de cine es un avance significativo en el campo de la interacci√≥n humano-m√°quina (HCI). Estos sistemas permiten adaptar din√°micamente la experiencia del usuario en funci√≥n de su estado emocional, lo que favorece una comunicaci√≥n m√°s natural y emp√°tica entre el ser humano y la tecnolog√≠a. En el contexto del entretenimiento, esto podr√≠a traducirse en una mayor inmersi√≥n y disfrute, ya que el contenido se ajusta autom√°ticamente a las respuestas emocionales del espectador. Este enfoque se alinea con los postulados del affective computing, un √°rea de la HCI que pretende endosar a los sistemas computacionales el algoritmo para reconocer, interpretar y responder a emociones humanas, y que por lo tanto mejora la calidad de interacci√≥n y la satisfacci√≥n del usuario [12].

Al considerar la utilizaci√≥n de tecnolog√≠as que monitorean emociones, se alude a beneficios potenciales por lo que respecta a la personalizaci√≥n y al bienestar, pero tambi√©n se levantaron cuestiones al respecto de su impacto psicol√≥gico. La continua evaluaci√≥n emocional puede tener efectos secundarios, como fatiga cognitiva, sensaci√≥n de vigilancia o dependencia de validaciones externas por parte del sistema. Por otro lado, tambi√©n se corre el riesgo de que los usuarios cambien su comportamiento de manera inconsciente al sentirse observados, lo que podr√≠a alterar la autenticidad de sus reacciones. Es necesario tener en cuenta estos factores para dise√±ar sistemas que no solo sean eficientes a nivel tecnol√≥gico, sino tambi√©n emocionalmente sostenibles. Por lo tanto, corresponde el establecimiento de mecanismos de control, consentimiento informado y retroalimentaci√≥n del usuario que fomenten una relaci√≥n √©tica y saludable con estas tecnolog√≠as [13].

Por √∫ltimo este proyecto tiene futuro en el an√°lisis emocional en salas de cine, que puede tener la capacidad de analizar emociones en tiempo real de espectadores y tener una personalizaci√≥n del contenido. Por √∫ltimo, el sistema es potencial para ser utilizado a otros campos de aplicaci√≥n, tales como la educaci√≥n o el bienestar mental, donde el seguimiento emocional tiene una influencia benefactora para el bienestar de las personas [14].

---

# Problema
En la actualidad, el an√°lisis autom√°tico de emociones ha ganado relevancia en sectores como la publicidad, la salud mental, la educaci√≥n y el entretenimiento. Particularmente en la industria del cine, comprender las reacciones emocionales de los espectadores ante determinados est√≠mulos visuales puede aportar informaci√≥n valiosa para el desarrollo de contenido m√°s atractivo y personalizado [1]. No obstante, los m√©todos tradicionales de recolecci√≥n de estas emociones (encuestas post-funci√≥n, entrevistas, etc.) resultan ser poco precisos, no escalables y altamente dependientes de la memoria y predisposici√≥n del espectador. Esta limitaci√≥n impide una evaluaci√≥n en tiempo real y en condiciones naturales, donde las emociones espont√°neas son clave [2].

Para solventar esta situaci√≥n, se requiere el desarrollo de un sistema embebido que, de manera aut√≥noma y en tiempo real, capture y clasifique las expresiones faciales de los espectadores durante la proyecci√≥n de una pel√≠cula. Este sistema deber√° operar bajo un esquema de edge computing (Edge AI), procesando localmente la informaci√≥n capturada para minimizar la latencia y proteger la privacidad del usuario. El reto t√©cnico incluye integrar hardware compacto (Raspberry Pi), herramientas de visi√≥n por computador (OpenCV), modelo de redes neuronales optimizadas para dispositivos embebidos (TensorFlow Lite) y una imagen de sistema operativo personalizada generada con Yocto Project, que contemple todas las dependencias y configuraciones necesarias para la operaci√≥n [3].

El sistema debe reconocer al menos seis emociones b√°sicas: enojo, felicidad, tristeza, miedo, disgusto y sorpresa. Tambi√©n debe registrar la actividad detectada mediante estampas de tiempo y transmitir los datos de forma segura hacia un sistema remoto o local para su posterior an√°lisis [4]. Todo este procesamiento debe llevarse a cabo localmente para minimizar la latencia y optimizar el uso del ancho de banda.

Este proyecto representa un caso de estudio realista y desafiante en el dise√±o de soluciones inteligentes, distribuidas y embebidas, con aplicaci√≥n directa en contextos del mundo real como las salas de cine, y potencial de escalabilidad hacia otros entornos sensibles al an√°lisis emocional.

---

# Requerimientos

Entre los Requerimientos funcionales del sistema a implementar, se debe cumplir con lo siguiente:

## ‚úÖ 1. Requerimientos Funcionales (RF)
| ID  | Nombre                        | Descripci√≥n                                                                                     |
| --- | ----------------------------- | ----------------------------------------------------------------------------------------------- |
| RF1 | Captura de im√°genes           | El sistema debe capturar im√°genes de los rostros de los espectadores utilizando una c√°mara USB. |
| RF2 | Clasificaci√≥n de emociones    | El sistema debe clasificar emociones en enojo, disgusto, miedo, felicidad, tristeza y sorpresa. |
| RF3 | Procesamiento local (Edge AI) | El procesamiento de im√°genes y clasificaci√≥n debe realizarse localmente en la Raspberry Pi 5.   |
| RF4 | Almacenamiento de resultados  | El sistema debe almacenar los resultados de la clasificaci√≥n por nodo con marca temporal.       |
| RF5 | Comunicaci√≥n en red           | Los nodos deben enviar sus resultados a un servidor central de manera peri√≥dica.                |
| RF6 | No intrusi√≥n al ambiente      | El sistema no debe perturbar la experiencia de los espectadores ni ser visualmente evidente.    |
| RF7 | Visualizaci√≥n remota         | El sistema debe permitir la visualizaci√≥n de los resultados de emociones desde una interfaz en un servidor o PC. |


## ‚úÖ 2. Requerimientos No Funcionales (RNF)
Es importante aclarar que los **Requerimientos No Funcionales** hacen referencias a **restricciones o caracter√≠sticas de calidad que debe cumplir el sistema**, siendo "una m√©trica" para evaluar el rendimiento del sistema empleado. 
| ID   | Nombre                     | Descripci√≥n                                                                 |
| ---- | -------------------------- | --------------------------------------------------------------------------- |
| RNF1 | Tiempo de respuesta        | El sistema debe clasificar emociones en menos de 2 segundos por imagen.     |
| RNF2 | Precisi√≥n del modelo       | El modelo de emociones debe tener al menos 80% de precisi√≥n en validaci√≥n.  |
| RNF3 | Uso de recursos limitado   | El sistema debe funcionar con un m√°ximo del 70% de CPU y 512 MB de RAM.     |
| RNF4 | Portabilidad               | El sistema debe ser desplegable en Raspberry Pi 5.                          |
| RNF5 | Privacidad de los usuarios | No se debe almacenar im√°genes completas ni datos personales identificables. |
| RNF6 | Requerimientos de energ√≠a   | Cada nodo debe poder alimentarse con una fuente de 5V 3A mediante USB-C. |


## ‚úÖ 3. Requerimientos del Sistema (Hardware y Software)
Entre lo planeado para el hardware a utilizar se proponen los siguientes elementos:

| Tipo          | Requerimiento                                                                                                                                                                                         |
| ------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Hardware**  | - Raspberry Pi 5 (8GB RAM)  <br> - Webcam USB tipo UVC  <br> - Fuente de alimentaci√≥n 5V 3A USB-C <br> - Conectividad Wi-Fi o Ethernet                                       |
| **Software**  | - Linux embebido generado con **Yocto Project** <br> - Python 3.10+ <br> - OpenCV 4+ <br> - TensorFlow Lite (tflite-runtime) <br> - Sistema de comunicaci√≥n cliente-servidor para env√≠o de resultados |
| **Bibliotecas** | - Numpy <br> - OpenCV <br> - TensorFlow Lite Runtime <br> - imutils <br> - PyUSB                                                                          |
| **Dataset**   | - Bases de datos de emociones faciales etiquetadas como:  <br> ‚Üí FER2013  <br> ‚Üí RAF-DB  <br> ‚Üí CK+ (Cohn-Kanade+)                                                                                    |




---

# Vista Operacional
A continuaci√≥n, se muestra un diagrama de la vista operacional del sistema:

```mermaid
flowchart TD
    subgraph Sala de Cine
        A[üéüÔ∏è Espectadores entran al cine]
        B[üé• Inicia la pel√≠cula]
    end

    subgraph Operador
        C[üßë‚Äçüíª Inicia el sistema]
    end

    subgraph C√°mara
        CAM1[üì∑ Captura emociones]
    end

    subgraph Raspberry Pi
        R1[üì• Solicita imagen a c√°mara]
        R2[üß† Clasificaci√≥n de emociones]
        R3[üíæ Guarda emociones + timestamp]
        R4[üì§ Env√≠a reporte]
    end

    subgraph Computador del Operador
        D[üì• Recepci√≥n del reporte]
        E[üìä Visualizaci√≥n/An√°lisis de emociones]
    end

    A --> B
    B --> C
    C --> R1
    R1 --> CAM1
    CAM1 --> R2
    R2 --> R3
    R3 --> R4
    R4 --> D
    D --> E

```

## üß© Descripci√≥n de la vista operacional
En este punto se presenta una descripci√≥n del comportamiento del sistema con el fin de que cualquier persona pueda entender su funcionamiento. Es importante que el modelo defina una perspectiva completamente operacional, sencilla y comprensible para el usuario. Por esta raz√≥n, se realiza una representaci√≥n de alto nivel, en la cual se destacan los componentes principales del sistema y la manera en que se utilizan de forma secuencial para lograr la detecci√≥n y registro de emociones. Esta representaci√≥n permite visualizar el flujo de operaci√≥n desde la llegada del espectador hasta la recopilaci√≥n final de datos por parte del operador.

- üéüÔ∏è Llegada del p√∫blico: Los espectadores ingresan a la sala de cine y se acomodan en sus respectivas butacas para disfrutar de la pel√≠cula.

- üßë‚Äçüíª Operador: Cuando inicia la pel√≠cula, el operador activa el sistema de detecci√≥n de emociones. Para ello, env√≠a un comando a la Raspberry Pi a trav√©s de la red WiFi, orden√°ndole que encienda la c√°mara y comience el procesamiento.

- üü¶ C√°mara USB: La cam√°ra se conecta a la Raspberry Pi, esta comienza a capturar im√°genes de los rostros del p√∫blico a raz√≥n de 1 fotograma por segundo (1 fps), y env√≠a las capturas en tiempo real a la Pi.

- üü© Raspberry Pi 5: La raspberry recibe las im√°genes capturadas por la c√°mara y la clasifica para determinar el estado emocional de la persona, por ultimo guarda la imagen mediante un archivo que contiene la emoci√≥n y el tiempo en que el rostro fue capturado.

- üü• Computador del Operador: Cuando termina la funci√≥n o el operador decide finalizar el monitoreo, este env√≠a otro comando a la Raspberry Pi para detener el sistema. Los archivos generados (emociones + timestamps) se transfieren autom√°ticamente al computador del operador mediante protocolo SSH, aprovechando que ambos dispositivos est√°n conectados a la misma red WiFi.

## üéØ Diagrama de casos de uso
A partir de las funciones que desempe√±an el espectador y el operador, es posible construir el diagrama de casos de uso. El espectador simplemente se sienta y disfruta de la funci√≥n de cine, mientras que el operador se encarga de varias funciones importantes como iniciar y finalizar la ejecuci√≥n de la aplicaci√≥n, adem√°s de analizar los datos obtenidos y generar reportes.

Cabe destacar que la elecci√≥n de capturar las im√°genes a una tasa de 1 cuadro por segundo (1 fps) se debe a que las emociones de corta duraci√≥n ‚Äîque suelen ser las expresiones m√°s comunes al visualizar una pel√≠cula‚Äî tienden a mantenerse durante varios segundos. Por ello, este intervalo de captura resulta suficiente para registrar dichas emociones de manera efectiva [15].

```mermaid
flowchart TD

    subgraph Eventos
        A[Se sienta en la sala de cine y visualiza la pel√≠cula]
        B[Inicia la ejecuci√≥n del sistema]
        C[Finaliza la ejecuci√≥n del sistema]
        D[Analiza los datos del sistema]
        E[Genera un reporte con datos obtenidos]
    end

    subgraph Espectador
        S[Espectador]
    end

    subgraph Operador
        O[Operador]
    end


    S --> A
    O --> B
    O --> C
    O --> D
    O --> E



```

## üïí Diagrama de secuencia
Seguidamente se presenta el diagrama de secuencia del sistema, en el que se consideran cinco elementos principales: el espectador, la c√°mara, la Raspberry Pi, el operador y el computador del operador. En esta secuencia, el espectador se limita a disfrutar de la pel√≠cula sin intervenir en el proceso. El operador se encarga de inicializar el sistema, lo que activa la interfaz e inicia el programa de detecci√≥n de emociones. A partir de este punto, la c√°mara captura im√°genes que son procesadas por la Raspberry Pi, donde tambi√©n se almacenan los archivos generados. Este ciclo se repite de forma continua hasta que el operador decide finalizar la ejecuci√≥n. Finalmente, los datos son transferidos al computador, donde la interfaz permite visualizar los resultados en formato de texto o gr√°ficos.

<p align="center">
  <img src="../imag/secuencia.jfif"  width="800"/>
</p>




---

# Vista Funcional
En esta apartado se analizar√° la interacci√≥n de los componentes del sistema y sus funciones clave.

## 1. Entrada: Captura de Datos
- **Funci√≥n**: Capturar im√°genes faciales de los espectadores.

- **Dispositivo**: C√°mara USB conectada a la Raspberry Pi.

- **Descripci√≥n**: La c√°mara, instalada discretamente en los respaldos de los asientos, toma im√°genes de los rostros en tiempo real, sin invadir la privacidad ni distraer al espectador.

## 2. Preprocesamiento de Im√°genes
- **Funci√≥n**: Preparar las im√°genes para la clasificaci√≥n.

- **Software**: OpenCV.

- **Descripci√≥n**: Se aplican transformaciones como redimensionamiento, normalizaci√≥n y conversi√≥n a escala de grises. Esto optimiza el rendimiento del modelo y reduce la carga de procesamiento.


## 3. Clasificaci√≥n de Emociones
- **Funci√≥n**: Detectar la emoci√≥n del espectador en cada imagen capturada.

- **Modelo**: Modelo ligero de aprendizaje profundo entrenado con bases como FER2013, RAF-DB o CK+.

- **Emociones clasificadas**: Enojo, disgusto, miedo, felicidad, tristeza y sorpresa.

- **Descripci√≥n**: El modelo, ejecutado localmente mediante TensorFlow Lite, clasifica emociones en menos de 2 segundos por imagen, utilizando la capacidad de procesamiento de la Raspberry Pi (Edge AI), sin depender de la nube.

## 4. Almacenamiento y Registro Local
- **Funci√≥n**: Guardar temporalmente los resultados de la clasificaci√≥n con marca de tiempo.

- **Dispositivo**: Almacenamiento local en la Raspberry Pi.

- **Descripci√≥n**: Los datos se almacenan como archivos estructurados o en una base de datos ligera para su posterior consulta y respaldo.

## 5. Env√≠o de Resultados al Servidor
- **Funci√≥n**: Transmitir los resultados peri√≥dicamente a un servidor central o computador central.

- **Protocolo**: HTTP, MQTT o WebSocket.

- **Descripci√≥n**: Los datos son enviados por red local (Wi-Fi o Ethernet). Idealmente la transmisi√≥n deber√≠a ser eficiente y no interferir con el desempe√±o del sistema embebido.

## 6. Visualizaci√≥n e Interacci√≥n
- **Funci√≥n**: Mostrar los resultados en una interfaz gr√°fica accesible para el operador.

- **Software**: Interfaz local personalizada.

- **Descripci√≥n**: Los datos se presentan de forma visual, permitiendo un an√°lisis en tiempo real o hist√≥rico del comportamiento emocional del p√∫blico. Se puede incluir una funci√≥n de activaci√≥n/desactivaci√≥n del sistema.

---

# Diagrama de Flujo de Reconocimiento y clasificacion de emociones

```mermaid
graph TD
    A[Reacci√≥n del Espectador] -->|Captura| B[C√°mara USB]
    B -->|Imagen capturada| C[Raspberry Pi 5]
    C -->| Preprocesamiento| D[OpenCV]
    D -->| Inferencia| E[TensorFlow Lite - Edge AI]
    E -->| Clasificaci√≥n de Emoci√≥n| F[Resultados con timestamp]
    F -->| Comunicaci√≥n| G[Env√≠a por Wi-Fi/Ethernet]
    G -->| Recepci√≥n| H[Servidor Central]
    H -->| Visualizaci√≥n| I[Interfaz Gr√°fica]
    I -->|Activaci√≥n/Desactivaci√≥n| J[Control de Ciclo]
    J -->| Retorno al procesamiento| C

```

## üìò Descripci√≥n funcional del flujo del sistema
El sistema comienza con la C√°mara USB, que se encarga de la captura de im√°genes o video en tiempo real. Estas im√°genes se env√≠an directamente a la Raspberry Pi 5, que act√∫a como el n√∫cleo de procesamiento local.

En la Raspberry Pi, las im√°genes pasan primero por una etapa de preprocesamiento utilizando OpenCV, donde se realizan operaciones como redimensionamiento, conversi√≥n a escala de grises y limpieza de ruido para optimizar el an√°lisis posterior. A continuaci√≥n, las im√°genes preprocesadas son procesadas mediante TensorFlow Lite, lo que permite ejecutar modelos de inteligencia artificial livianos directamente en el dispositivo.

Posteriormente, el sistema realiza la clasificaci√≥n emocional utilizando un modelo de Edge AI, que detecta emociones como felicidad, tristeza, enojo, entre otras. Los resultados de esta inferencia se almacenan localmente junto con una marca temporal para futuras referencias o auditor√≠a.

Luego, los datos son empaquetados y enviados v√≠a Wi-Fi o Ethernet al Servidor Central, donde se realiza la visualizaci√≥n de los resultados. La informaci√≥n es desplegada en una interfaz gr√°fica amigable, permitiendo a los operadores monitorear en tiempo real o analizar datos hist√≥ricos.

Finalmente, desde esta interfaz tambi√©n se puede ejercer un control del ciclo del sistema, que puede ajustar par√°metros o reiniciar el proceso. Esta orden de control se retroalimenta a la Raspberry Pi, reiniciando el flujo de procesamiento a partir del preprocesamiento.


---

# Arquitectura
La arquitectura del sistema combinar√° componentes de hardware y software para capturar, procesar, clasificar y visualizar las emociones de los espectadores en tiempo real. A continuaci√≥n, se describe c√≥mo cada funci√≥n del sistema es ejecutada por un conjunto espec√≠fico de componentes.

## Descripci√≥n General
El sistema est√° compuesto por los siguientes bloques funcionales:

### 1. Captura de Im√°genes

- **Funci√≥n**: Obtener im√°genes faciales del espectador.

- **Hardware**: C√°mara USB tipo UVC.

- **Interfaz**: USB.

### 2. Procesamiento y Clasificaci√≥n de Emociones (Edge AI)

- **Funci√≥n**: Preprocesar im√°genes y detectar emociones.

- **Hardware**: Raspberry Pi 5.

- **Software**: OpenCV (preprocesamiento), TensorFlow Lite (procesamiento), Modelo (Edge AI).

- **Interfaz**: Interna (entre m√≥dulos de software en la Raspberry Pi).

### 3. Almacenamiento Temporal

- **Funci√≥n**: Registrar resultados con marca de tiempo.

- **Hardware**: Memoria local (SD de almacenamiento interno de la Raspberry Pi).

- **Software**: Python (manejo de archivos).


### 4. Comunicaci√≥n y Transmisi√≥n de Resultados

- **Funci√≥n**: Enviar datos al servidor central.

- **Hardware**: M√≥dulo de red de la Raspberry Pi (Wi-Fi o Ethernet).

- **Software**: Cliente HTTP, MQTT o WebSocket implementado en Python.

### 5. Visualizaci√≥n de Datos

- **Funci√≥n**: Mostrar los resultados de forma gr√°fica para su an√°lisis.

- **Hardware**: Servidor central (PC o nube).

- **Software**: Dashboard en Grafana, Kibana o una aplicaci√≥n web.

## Diagrama de Arquitectura del Sistema

```mermaid
flowchart TB

    subgraph Camara USB
        A[Entrada de video o Captura de im√°genes]
    end

    subgraph Raspberry Pi 5
        B[Preprocesamiento con OpenCV]
        C[Procesamiento con TFLite]
        D[Clasificaci√≥n con Modelo Edge IA]
        E[Almacenamiento de respuesta local]
        F[Env√≠o de paquetes v√≠a WiFi o Ethernet]
    end

    subgraph Servidor Central
        G[Visualizaci√≥n]
        H[Interfaz Gr√°fica]
        I[Control de Ciclo]
    end

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> B

```
## üìò Descripci√≥n funcional del flujo del sistema
El sistema inicia con una c√°mara USB que captura im√°genes en tiempo real, para luego ser enviadas a una Raspberry Pi 5, donde se preprocesan con OpenCV y luego se analizan usando TensorFlow Lite con un modelo de edge IA embebida para reconocimiento emocional. Las emociones detectadas (felicidad, tristeza, enojo, etc.) se guardan con marca temporal y se transmiten por Wi-Fi o Ethernet a un servidor o computador central, donde se visualizan en una interfaz gr√°fica. Esta interfaz permite el monitoreo en tiempo real.


---

# Dependencias
Dentro de las dependencias generales para la imagen del sistema operativo de Yocto.

## üå≤ √Årbol de Dependencias de Capas Yocto (general)
```plaintext
poky
‚îÇ
‚îú‚îÄ‚îÄ meta-poky (distribuci√≥n de referencia)
‚îú‚îÄ‚îÄ meta-yocto-bsp (BSPs de referencia)
‚îú‚îÄ‚îÄ meta-openembedded
‚îÇ   ‚îú‚îÄ‚îÄ meta-oe (paquetes b√°sicos y utilidades)
‚îÇ   ‚îú‚îÄ‚îÄ meta-python (soporte para Python y paquetes relacionados)
‚îÇ   ‚îú‚îÄ‚îÄ meta-multimedia (paquetes multimedia: codecs, players, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ meta-networking (paquetes de red y protocolos)
‚îÇ
‚îú‚îÄ‚îÄ meta-raspberrypi (BSP espec√≠fico para RPi)
‚îî‚îÄ‚îÄ meta-mylayer (capa personalizada del usuario para la app DNN)

```

## üìã An√°lisis general de las dependencias:
`meta y meta-poky`: proporcionan el n√∫cleo del sistema de construcci√≥n (Poky) y la distribuci√≥n base de referencia.

`meta-yocto-bsp`: incluye soporte para placas de hardware de referencia para la Raspberry Pi.

`meta-openembedded`: agrupaci√≥n clave que extiende las capacidades de Poky.

`meta-oe`: utilidades b√°sicas, bibliotecas comunes.

`meta-python`: dependencias de Python.

`meta-multimedia`: soporte para procesamiento de video/audio.

`meta-networking`: gesti√≥n de interfaces y protocolos de red.

`meta-raspberrypi`: contiene todo lo necesario para generar im√°genes compatibles con Raspberry Pi.

`meta-mylayer`: es la capa personalizada que incluye recetas para la aplicaci√≥n de inferencia, los scripts y las dependencias espec√≠ficas.



---

# Integraci√≥n

El diagrama de la arquitectura integrada de software y hardware es: 

```mermaid
flowchart TB
    subgraph Espectador
        P[üòÉ Reacci√≥n del Espectador]
    end

    subgraph Camara USB
        A[üé• Captura de im√°genes]
    end

    subgraph Raspberry Pi 5
        B[üß† Preprocesamiento OpenCV]
        C[üîç Inferencia con TFLite]
        D[üè∑Ô∏è Clasificaci√≥n con Modelo ]
        E[üíæ Almacenamiento local]
        F[üì° Env√≠o de datos ]

        subgraph Sistema Yocto
            Y1[meta-poky]
            Y2[meta-yocto-bsp]
            Y3[meta-raspberrypi]
            Y4[meta-openembedded]
            Y5[meta-mylayer]
        end
    end

    subgraph Servidor Central
        G[üñ•Ô∏è Visualizaci√≥n resultados]
        H[üß© Interfaz de control]
        I[üîÅ Control de ciclo remoto]
    end

    subgraph Operador
        O[üë§ Interacci√≥n y monitoreo]
    end

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> B
    O --> H
    P --> A

    %% Colores suaves y distintos para cada capa
    style Y1 fill:#dbeafe,stroke:#3b82f6,color:#1e3a8a
    style Y2 fill:#fef3c7,stroke:#f59e0b,color:#78350f
    style Y3 fill:#ede9fe,stroke:#8b5cf6,color:#4c1d95
    style Y4 fill:#dcfce7,stroke:#22c55e,color:#14532d
    style Y5 fill:#ffe4e6,stroke:#e11d48,color:#881337,font-weight:bold



```


## üß© Descripci√≥n del proceso de integraci√≥n hardware/software
La soluci√≥n implementada combina de forma coordinada componentes de hardware y software para lograr un sistema funcional de detecci√≥n y visualizaci√≥n de emociones. A continuaci√≥n se detalla c√≥mo se integran y complementan estos componentes en cada etapa del proceso:

- üòÉ Reacci√≥n del Espectador: Es la reacci√≥n o la expresi√≥n de la emoci√≥n por parte del espectador

- üü¶ C√°mara USB (hardware): Es el punto de entrada del sistema. Captura im√°genes en tiempo real del espectador o usuario cuya expresi√≥n emocional ser√° analizada.

- üü© Raspberry Pi 5 (hardware + software): Act√∫a como nodo de procesamiento en el borde. Recibe la imagen desde la c√°mara y ejecuta varios procesos:

  - OpenCV (software): Realiza el preprocesamiento de la imagen (detecci√≥n de rostro, recorte, escalado y normalizaci√≥n).

  - TensorFlow Lite (software): Ejecuta el modelo de aprendizaje autom√°tico entrenado para clasificar emociones en tiempo real.

  - Red de comunicaci√≥n (hardware/software): Se utiliza WiFi o Ethernet para transferir los resultados inferidos (emociones y timestamps) al servidor central.

- üü• Servidor o computador Central (hardware + software): Recibe los datos procesados desde la Raspberry Pi y permite la interacci√≥n con el operador:

  - Interfaz Gr√°fica (software): Presenta visualmente las emociones detectadas y permite al operador ajustar par√°metros del sistema (por ejemplo, encender/apagar el ciclo de inferencia o cambiar umbrales de confianza).

  - Control remoto del ciclo de procesamiento (software): El operador puede modificar el comportamiento del sistema en tiempo real, enviando instrucciones de vuelta a la Raspberry Pi.

Este dise√±o modular e integrado permite una interacci√≥n fluida entre hardware y software: la Raspberry Pi se encarga de la inferencia local, optimizando la latencia y reduciendo la carga sobre la red, mientras que el servidor central act√∫a como centro de control y visualizaci√≥n, facilitando ajustes remotos y monitoreo continuo.


---

# Planificaci√≥n
Para garantizar una ejecuci√≥n ordenada y efectiva del proyecto, se realiz√≥ una planificaci√≥n estructurada que contempla la asignaci√≥n de roles, la divisi√≥n de tareas y la definici√≥n de entregables asociados a cada fase del desarrollo. Esta planificaci√≥n est√° representada en un diagrama de Gantt, el cual permite visualizar la secuencia l√≥gica y temporal de las actividades. A continuaci√≥n, se presenta la lista de tareas principales que guiar√°n el desarrollo del sistema embebido:

- Revisi√≥n t√©cnica y conceptual del problema: Esta etapa inicial est√° orientada a comprender en profundidad los objetivos del proyecto, las emociones a clasificar, y los requisitos funcionales y no funcionales del sistema. Incluye adem√°s el estudio preliminar de las tecnolog√≠as involucradas.
- Dise√±o y documentaci√≥n de la propuesta t√©cnica: Se elabora el documento de propuesta de dise√±o, el cual define la arquitectura general del sistema, los componentes a integrar, y los criterios de operaci√≥n bajo un esquema de Edge AI.
- Desarrollo inicial del modelo de clasificaci√≥n de emociones: Se implementa un modelo de reconocimiento facial de emociones en Python, utilizando un entorno de escritorio como plataforma de prueba. Esta versi√≥n permitir√° validar la l√≥gica del modelo y realizar ajustes antes de optimizarlo.
- Incorporaci√≥n de funcionalidad de registro: Se a√±ade al sistema la capacidad de registrar los resultados de la clasificaci√≥n junto con sus respectivas marcas de tiempo, en un formato estructurado para facilitar su an√°lisis posterior.
- Conversi√≥n y validaci√≥n del modelo con TensorFlow Lite: El modelo desarrollado se convierte a TensorFlow Lite para reducir su tama√±o y mejorar su rendimiento en entornos con recursos limitados. Posteriormente se realizan pruebas de compatibilidad y precisi√≥n.
- Configuraci√≥n de capas Yocto necesarias para el sistema: Se configuran e integran las capas meta-tensorflow-lite, meta-openembedded, meta-python y meta-raspberrypi, necesarias para compilar una imagen personalizada de Linux compatible con Raspberry Pi y con las dependencias requeridas.
- Desarrollo de capa personalizada meta-edgeAI: Se crea una capa espec√≠fica que contenga tanto el modelo convertido como el c√≥digo necesario para ejecutarlo dentro del sistema embebido.
- Generaci√≥n y prueba de la imagen embebida en entorno virtual: Se genera la imagen del sistema operativo mediante Yocto y se prueba en un entorno de m√°quina virtual utilizando videos como insumo para simular la entrada de c√°mara.
- Despliegue y verificaci√≥n en hardware f√≠sico: La imagen generada se despliega en una Raspberry Pi 4, conectada a una c√°mara. Se valida el funcionamiento del sistema en condiciones reales, verificando detecci√≥n, registro y procesamiento local.
- Implementaci√≥n de la comunicaci√≥n remota: Se establece un canal de comunicaci√≥n seguro entre la Raspberry Pi y un equipo remoto utilizando SSH, permitiendo la transferencia de reportes y el control del sistema de forma externa.
- Desarrollo de una interfaz operativa b√°sica: Se crea una interfaz de usuario simple que facilite el arranque, monitoreo y cierre del sistema, as√≠ como la consulta de los reportes generados.
- Integraci√≥n final y pruebas completas del sistema: Se realiza una verificaci√≥n integral que incluya detecci√≥n en tiempo real, registro local, comunicaci√≥n remota, y funcionamiento continuo en la Raspberry Pi, asegurando la coherencia con los objetivos del proyecto.

## Diagrama de Gantt
<p align="center">
  <img src="../imag/diagrama.jpg"  width="1000"/>
</p>


---

# Conclusiones
El desarrollo de un sistema embebido para el reconocimiento y clasificaci√≥n de expresiones faciales en una sala de cine es una soluci√≥n innovadora basada en tecnolog√≠as de Edge AI para el an√°lisis emocional en tiempo real. El proyecto integra de forma efectiva hardware y software especializados, incluyendo plataformas como Raspberry Pi, herramientas de visi√≥n por computador como OpenCV, modelos optimizados con TensorFlow Lite y entornos de construcci√≥n personalizados como Yocto Project. La capacidad del sistema para identificar emociones como enojo, felicidad, tristeza y sorpresa evidencia su aplicabilidad en entornos interactivos, destacando su potencial en sectores como el entretenimiento, la experiencia del usuario y el an√°lisis de comportamiento humano.
Adem√°s, el trabajo destaca la importancia de una metodolog√≠a de desarrollo estructurada, con una gesti√≥n adecuada de requerimientos, una clara definici√≥n de roles dentro del equipo, y un an√°lisis detallado de dependencias para la integraci√≥n eficiente de hardware y software. La implementaci√≥n pr√°ctica evidencia la viabilidad t√©cnica del sistema, as√≠ como su capacidad para ofrecer soluciones funcionales y confiables en escenarios del mundo real. Este proyecto refuerza el valor de la planificaci√≥n colaborativa y la innovaci√≥n tecnol√≥gica en el √°mbito de los sistemas embebidos.


---

# Referencias

[1] T. -L. Liao, Y. -Y. Hou and Y. -H. Tseng, "Movie Recommendation System Based on Facial Emotion Recognition," 2024 IEEE International Conference on Future Machine Learning and Data Science (FMLDS), Sydney, Australia, 2024, pp. 141-146, doi: 10.1109/FMLDS63805.2024.00036.

[2] D. Shukla, R. Kumari and A. Bhargavi, "Human Face Detection and Emotion Recognition Using OpenCV through AI," 2024 12th International Conference on Internet of Everything, Microwave, Embedded, Communication and Networks (IEMECON), Jaipur, India, 2024, pp. 1-5, doi: 10.1109/IEMECON62401.2024.10845980.

[3] S. Turabzadeh, H. Meng, R. M. Swash, M. Pleva and J. Juhar, "Real-time emotional state detection from facial expression on embedded devices," 2017 Seventh International Conference on Innovative Computing Technology (INTECH), Luton, UK, 2017, pp. 46-51, doi: 10.1109/INTECH.2017.8102423.

[4] M. Srinivas; Sanjeev Saurav; Akshay Nayak; A. P. Murukessan, "Facial Expression Recognition Using Fusion of Deep Learning and Multiple Features," in Machine Learning Algorithms and Applications , Wiley, 2021, pp.229-246, doi: 10.1002/9781119769262.ch13.

[5] Matsumoto, D., Hwang, H. S., L√≥pez, R. M., & P√©rez-Nieto, M. √Å. (2013). Lectura de la expresi√≥n facial de las emociones: Investigaci√≥n b√°sica en la mejora del reconocimiento de emociones. Ansiedad y estr√©s.

[6] Lee, J. R. H., & Wong, A. (2020). AEGIS: A real-time multimodal augmented reality computer vision based system to assist facial expression recognition for individuals with autism spectrum disorder.

[7] Khanzada, A., Bai, C., & Celepcikay, F. T. (2020). Facial expression recognition with deep learning.

[8] R. Turabzadeh, L. Meng, S. Swash, M. Pleva, and M. Juhar, "Optimizing neural networks for embedded systems: TensorFlow Lite and Raspberry Pi," Embedded Systems Journal, vol. 13, no. 2, pp. 75-88, 2017.

[9] J. Wang, M. Chen, and H. Zhao, "Facial emotion recognition using deep learning techniques," IEEE Transactions on Artificial Intelligence, vol. 5, no. 2, pp. 56-65, 2021.

[10] Y. Wang, Z. Liu, and X. Yang, "Data security in real-time emotion recognition systems," Journal of Information Security, vol. 34, no. 1, pp. 101-110, 2021.

[11] T. Cheng, X. Zhang, and P. Yang, "Yocto Project for embedded system customization," IEEE Embedded Systems Conference, pp. 45-51, 2017.

[12] Picard, R. W. (1997). Affective Computing. MIT Press.

[13] Calvo, R. A., & D'Mello, S. (2010). Affect detection: An interdisciplinary review of models, methods, and their applications. IEEE Transactions on Affective Computing, 1(1), 18‚Äì37. https://doi.org/10.1109/T-AFFC.2010.1

[14] A. T. Nelson, J. A. Glickman, and P. Sharma, "Scalable emotion recognition: Applications in education and mental health," IEEE Transactions on Affective Computing, vol. 12, no. 3, pp. 99-110, 2023.

[15] R. Kadakia, P. Kalkotwar, P. Jhaveri, R. Patanwadia and K. Srivastava, "Analysis of Micro Expressions using XAI," 2022 3rd International Conference on Computing, Analytics and Networks (ICAN), Rajpura, Punjab, India, 2022, pp. 1-7, doi: 10.1109/ICAN56228.2022.10007340.

---

